{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e32bc2f",
   "metadata": {},
   "source": [
    "# Contextualized Bayesian Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30c7901a",
   "metadata": {},
   "source": [
    "For more details, please see the [NOTMAD preprint](https://arxiv.org/abs/2111.01104)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5356f75f",
   "metadata": {},
   "source": [
    "Here, we want to ask the following questions:\n",
    "- how the performance of NOTMAD compares against population networks and cluster network methods.\n",
    "- how the performance of NOTMAD changes with loss type (\"NOTEARS\", \"DAGMA\", or \"poly\")\n",
    "- how the performance of NOTMAD changes with number of factors (\"num_factors\")\n",
    "- how the performance changes with number of samples (n) and number of features (p). \n",
    "\n",
    "TODO:\n",
    "- Automated generation of W (clusters, linear function of C)\n",
    "\n",
    "Possibly, vary:\n",
    "- signal-to-noise ratio of the context-to-network parameter relationship\n",
    "- encoder type (\"ngam\", \"mlp\")\n",
    "- regularization of NOTMAD:\n",
    "    NOTEARS loss has parameters:\n",
    "    \n",
    "        alpha (float)\n",
    "\n",
    "        rho (float)\n",
    "\n",
    "        use_dynamic_alpha_rho (Boolean)\n",
    "\n",
    "    DAGMA loss has parameters:\n",
    "    \n",
    "        alpha (strength, default 1e0)\n",
    "\n",
    "        s (max spectral radius, default 1)\n",
    "\n",
    "Report results in terms of:\n",
    "- MSE of X predictions (measure_mses function)\n",
    "- recovery of W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cdc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#auto update\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "import contextualized\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from contextualized.dags.graph_utils import simulate_linear_sem, is_dag\n",
    "from contextualized.baselines import (\n",
    "    BayesianNetwork,\n",
    "    GroupedNetworks,\n",
    ")\n",
    "from contextualized.easy import ContextualizedBayesianNetworks\n",
    "import pickle as pkl\n",
    "import os\n",
    "import numpy\n",
    "\n",
    "from ray import tune\n",
    "import logging\n",
    "import functools\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "\n",
    "import ray\n",
    "logging.getLogger(\"ray.tune\").setLevel(logging.FATAL)\n",
    "ray.init(\n",
    "    num_cpus=2,\n",
    "    num_gpus=1,\n",
    "    object_store_memory=2*1024*1024*1024,\n",
    "    log_to_driver=False,\n",
    "    logging_level=logging.FATAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mses(betas, X, individual_preds=False):\n",
    "    \"\"\"\n",
    "    Measure mean-squared errors.\n",
    "    \"\"\"\n",
    "    mses = np.zeros((len(betas), len(X)))  # n_bootstraps x n_samples\n",
    "    for bootstrap in range(len(betas)):\n",
    "        for target_feat in range(X.shape[-1]):\n",
    "            # betas are n_boostraps x n_samples x n_features x n_features\n",
    "            # preds[bootstrap, sample, i] = X[sample, :].dot(betas[bootstrap, sample, i, :])\n",
    "            preds = np.array(\n",
    "                [\n",
    "                    X[sample].dot(betas[bootstrap, sample, :, target_feat])  # + mus[bootstrap, j, i]\n",
    "                    for sample in range(len(X))\n",
    "                ]\n",
    "            )\n",
    "            residuals = X[:, target_feat] - preds\n",
    "            mses[bootstrap, :] += residuals**2 / (X.shape[-1])\n",
    "    if not individual_preds:\n",
    "        mses = np.mean(mses, axis=0)\n",
    "    return mses\n",
    "\n",
    "def measure_recovery(W_true, W):\n",
    "    # Assumes W has a prefix dimension for bootstraps\n",
    "    recovery_errs = []\n",
    "    for bootstrap in range(len(W)):\n",
    "        for sample in range(len(W[bootstrap])):\n",
    "            recovery_errs.append(np.linalg.norm(W_true[sample] - W[bootstrap][sample], ord=2))\n",
    "    return np.mean([recovery_errs])\n",
    "\n",
    "def make_dag(p, n_nonempty=4):\n",
    "    # # n_nonempty = number of non-empty cells in W\n",
    "    # coeffs = [np.random.choice([np.random.uniform(-1,-0.5), np.random.uniform(0.5,1)]) for i in range(n_nonempty)]\n",
    "    # c * coeffs[assign_c_idx.index((i,j))]\n",
    "    #create upper triangular\n",
    "    tuples = [(i, j) for i in range(p) for j in range(p) if i < j]\n",
    "    \n",
    "    #randomly flip to account for monodirectinality\n",
    "    tuples = [(t[1],t[0]) if np.random.uniform(0,1) > 0.5 else t for t in tuples]\n",
    "\n",
    "    #select n_nonempty\n",
    "    assign_c_idx = random.sample(tuples, n_nonempty)\n",
    "\n",
    "    w = np.zeros((p, p))\n",
    "\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            if (i,j) in assign_c_idx:\n",
    "                w[i, j] = 1\n",
    "                # w[0, 1] = 1\n",
    "                # w[2, 1] = 1\n",
    "                # w[3, 1] = 1\n",
    "                # w[3, 2] = 1\n",
    "    if is_dag(w):\n",
    "        return w\n",
    "    else:\n",
    "        return make_dag(p, n_nonempty)\n",
    "\n",
    "def generate_WC(p, n, n_clusters = 5, mode='linear'):\n",
    "    '''Generate W and C for a given p, n, and mode.'''\n",
    "\n",
    "    W = np.zeros((n, p, p))\n",
    "    \n",
    "    if mode == 'cluster':\n",
    "        \n",
    "        C = np.zeros((n, 1))\n",
    "        cs = [np.random.normal(0, 1) for i in range(n_clusters)]\n",
    "        centroid_ws = []\n",
    "        for i in range(n_clusters):\n",
    "            centroid_ws.append(make_dag(p))\n",
    "\n",
    "        for i in range(n):\n",
    "            c_idx = np.random.choice(len(centroid_ws))\n",
    "            masked_std = np.random.normal(0, 0.2, size=(p, p)) * (centroid_ws[c_idx] != 0)            \n",
    "            W[i] = centroid_ws[c_idx] + masked_std\n",
    "            C[i,0] = c_idx\n",
    "\n",
    "    elif mode == 'linear':\n",
    "        dag = make_dag(p)\n",
    "\n",
    "        C = np.random.normal(0, 1, size=(n, 1))\n",
    "        coeffs = [np.random.choice([np.random.uniform(-1,-0.5), np.random.uniform(0.5,1)]) for i in range(int(dag.sum()))]\n",
    "        \n",
    "        for i in range(n):\n",
    "            #Each non-empty = c * coeffs[assign_c_idx.index((i,j))]\n",
    "            assign_c_idx = [(i,j) for i in range(p) for j in range(p) if dag[i,j] == 1]\n",
    "            for j in range(len(assign_c_idx)):\n",
    "                W[i, assign_c_idx[j][0], assign_c_idx[j][1]] = C[i,0] * coeffs[j]\n",
    "\n",
    "    else:\n",
    "        assert \"Error, mode must be 'cluster' or 'linear'\"\n",
    "    \n",
    "    return W, C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac9ea56b",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sweep over graph types.\n",
    "# TODO: Sweep over factors\n",
    "# TODO: Sweep over signal-to-noise ratio.\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "data_dir = 'tuning_directory'\n",
    "\n",
    "os.makedirs(data_dir,exist_ok=True)\n",
    "\n",
    "def get_data():\n",
    "    n_data_gens = 1\n",
    "    ns = [100, 1000]\n",
    "    ps = [4, 10]\n",
    "    graph_types = [\"gauss\", \"exp\", \"gumbel\", \"uniform\", \"logistic\", \"poisson\"]\n",
    "    W_mode = ['cluster', 'linear']\n",
    "    n_clusters = 5\n",
    "    data = []\n",
    "    data_line = \"\"\n",
    "    # with open(\"data2/data_infodsfds.csv\", 'w') as out_file:\n",
    "    for n in ns:\n",
    "        for p in ps:\n",
    "            # for noise in noise_scales:\n",
    "                # for graph_type in graph_types:\n",
    "            for mode in W_mode:\n",
    "                for data_gen in range(n_data_gens):\n",
    "                    data_line = f\"{n}, {p}, {W_mode}, {data_gen}\"\n",
    "                    W, C = generate_WC(p, n, n_clusters, mode=mode)\n",
    "                    X = np.array([simulate_linear_sem(w, n_samples=1, sem_type='uniform', noise_scale=0.0)[0] for w in W])\n",
    "                    data.append((C, X, W))\n",
    "                    # print(data_line, file=out_file)\n",
    "    # # save data\n",
    "    pkl.dump(data, open(f\"{data_dir}/{data_dir}_data.pkl\", 'wb'))\n",
    "    return data\n",
    "\n",
    "# datas = get_data()\n",
    "datas = pkl.load(open(f\"{data_dir}/{data_dir}_data.pkl\", 'rb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "194f87c4",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c231342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, data, strat_config, checkpoint_dir='checkpoints/'):\n",
    "    C,X,W = data\n",
    "    \n",
    "    config.update(strat_config)\n",
    "\n",
    "    n = W.shape[0]\n",
    "    p = W.shape[-1]\n",
    "    k = config['ks']\n",
    "    n_fit_iters = 1\n",
    "\n",
    "    header = \"n, p, k, data_gen, fit_iter, \"\n",
    "    header += \"recovery_pop_train, recovery_pop_test, mse_pop_train, mse_pop_test, \"\n",
    "    header += \"recovery_cluster_train, recovery_cluster_test, mse_cluster_train, mse_cluster_test, \"\n",
    "    header += \"recovery_notmad_notears_train, recovery_notmad_notears_test, mse_notmad_notears_train, mse_notmad_notears_test, \"\n",
    "    header += \"recovery_notmad_dagma_train, recovery_notmad_dagma_test, mse_notmad_dagma_train, mse_notmad_dagma_test, \"\n",
    "    header += \"recovery_notmad_poly_train, recovery_notmad_poly_test, mse_notmad_poly_train, mse_notmad_poly_test\"\n",
    "\n",
    "    C_train, C_test, X_train, X_test, W_train, W_test = train_test_split(C, X, W, test_size=0.3)\n",
    "\n",
    "    recovery_train = lambda W_pred: measure_recovery(W_train, W_pred)\n",
    "    recovery_test = lambda W_pred: measure_recovery(W_test, W_pred)\n",
    "    mse_train = lambda W_pred: np.mean(measure_mses(W_pred, X_train))\n",
    "    mse_test = lambda W_pred: np.mean(measure_mses(W_pred, X_test))\n",
    "    results_string = lambda W_pred_train, W_pred_test: f\", {recovery_train(W_pred_train)}, {recovery_test(W_pred_test)}, {mse_train(W_pred_train)}, {mse_test(W_pred_test)}\"\n",
    "    \n",
    "    test_mses = []\n",
    "    test_recoveries = []\n",
    "    train_mses = []\n",
    "    train_recoveries = []\n",
    "    # results_line = \"\"\n",
    "    for fit_iter in range(n_fit_iters):\n",
    "\n",
    "        if config['loss_type'] == 'NOTEARS':\n",
    "            loss_kwargs = {\n",
    "                'archetype_alpha': 0.0,\n",
    "                'archetype_rho': 0.0,\n",
    "                'archetype_use_dynamic_alpha_rho':True,\n",
    "                'archetype_tol': config['tol'],\n",
    "                \n",
    "                'sample_specific_tol': config['tol'],\n",
    "                'sample_specific_alpha': config['alpha'],\n",
    "                'sample_specific_rho': config['rho'],\n",
    "                'sample_specific_use_dynamic_alpha_rho': True\n",
    "            }\n",
    "        elif config['loss_type'] == 'DAGMA':\n",
    "            loss_kwargs = {\n",
    "                'archetype_alpha': 0.0,\n",
    "                'archetype_s': 0.0,\n",
    "                'archetype_use_dynamic_alpha_rho':False,\n",
    "                'sample_specific_alpha': config['alpha'],\n",
    "                'sample_specific_s': config['s'],\n",
    "                'sample_specific_use_dynamic_alpha_rho': False,\n",
    "            }\n",
    "        elif config['loss_type'] == 'poly':\n",
    "            loss_kwargs = {}\n",
    "\n",
    "        cbn = ContextualizedBayesianNetworks(\n",
    "            encoder_type=config['encoder_types'],\n",
    "            num_archetypes=k,\n",
    "            num_factors=config['n_factors'],\n",
    "            archetype_dag_loss_type=config['loss_type'],\n",
    "            sample_specific_dag_loss_type=config['loss_type'],\n",
    "            **loss_kwargs,\n",
    "            n_bootstraps=3,\n",
    "            learning_rate=1e-3,\n",
    "        )\n",
    "        cbn.fit(C_train, X_train, max_epochs=100)\n",
    "        \n",
    "        cbn_preds_train = cbn.predict_networks(C_train, individual_preds=True)\n",
    "        cbn_preds_test = cbn.predict_networks(C_test, individual_preds=True)\n",
    "        \n",
    "        test_mses.append(mse_test(cbn_preds_test))\n",
    "        test_recoveries.append(recovery_test(cbn_preds_test))\n",
    "        train_mses.append(mse_train(cbn_preds_train))\n",
    "        train_recoveries.append(recovery_train(cbn_preds_train))\n",
    "    \n",
    "    train_mse = np.mean(train_mses)\n",
    "    train_recovery = np.mean(train_recoveries)\n",
    "    test_mse = np.mean(test_mses)\n",
    "    test_recovery = np.mean(test_recoveries)\n",
    "    \n",
    "    tune.report(train_mse=train_mse, train_recovery=train_recovery,test_mse=test_mse, test_recovery=test_recovery)\n",
    "    return cbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter groups\n",
    "losses = [\"NOTEARS\", \"DAGMA\", \"poly\"]\n",
    "encoder_types = ['ngam', 'mlp']\n",
    "ks = [4, 8, 16]\n",
    "\n",
    "stratify_by = {\n",
    "    'loss_type': losses,\n",
    "    'encoder_types': encoder_types,\n",
    "    'ks':ks\n",
    "}\n",
    "\n",
    "n_runs = 3\n",
    "data_dir = 'tuning_directory'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0908ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print NOTMAD params\n",
    "\n",
    "def debug_loop():\n",
    "    default_config = {\n",
    "        \"encoder_types\": \"ngam\",\n",
    "        'use_dynamic_alpha_rho': False,\n",
    "        \"ks\": 4,\n",
    "        \"tol\":0.1,\n",
    "        \"n_factors\": 2,\n",
    "        \"alpha\": 0.01,\n",
    "        \"rho\": 0.01,\n",
    "        \"s\": 0.65,\n",
    "    }\n",
    "    cbns = {}\n",
    "\n",
    "    for loss in [\"DAGMA\", \"NOTEARS\", \"poly\"]:\n",
    "        cf = default_config.copy()\n",
    "        cf['loss_type'] = loss\n",
    "        data = datas[0]\n",
    "        cbns[loss] = train_and_evaluate(cf, data, {})\n",
    "    \n",
    "    return cbns\n",
    "\n",
    "# cbns = debug_loop()\n",
    "\n",
    "# for loss, cbn in cbns.items():\n",
    "#     print(loss)\n",
    "#     print(f'ss params: {cbn.models[0].ss_dag_params}')\n",
    "#     print(f'arch params: {cbn.models[0].arch_dag_params}')\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195922d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3 x (500) num_models / samples] x [(2 + 3 + 2) hps] x [20 datasets] = 210,000 runs\n",
    "\n",
    "\n",
    "for d in range(len(datas)):\n",
    "    for strat in stratify_by.keys():\n",
    "        config = {\n",
    "            \"loss_type\": tune.choice(losses),\n",
    "            \"encoder_types\": tune.choice(encoder_types),\n",
    "            \"ks\": tune.choice(ks),\n",
    "            \"n_factors\": tune.sample_from(lambda spec: int(np.random.randint(0,spec.config.ks))),\n",
    "            \"alpha\": tune.uniform(0.001, 0.1),\n",
    "            \"rho\": tune.uniform(0.001, 0.1),\n",
    "            \"s\": tune.uniform(0.001, 5),\n",
    "        }\n",
    "        \n",
    "        del config[strat]\n",
    "\n",
    "        if strat == 'ks':\n",
    "            del config['n_factors']\n",
    "\n",
    "        from ray.tune.schedulers import ASHAScheduler\n",
    "        from ray.tune import CLIReporter\n",
    "        from ray.air import RunConfig\n",
    "\n",
    "        for instance in stratify_by[strat]:\n",
    "            if strat == 'ks':\n",
    "                config['n_factors'] = tune.sample_from(lambda spec: int(np.random.randint(0,instance)))\n",
    "            \n",
    "            for i in range (n_runs):\n",
    "                \n",
    "                if not os.path.exists(f'{data_dir}/'): os.mkdir(f'{data_dir}/')\n",
    "                if os.path.exists(f'{data_dir}/tune_analysis_d{d}_s{strat}_ins{instance}_i{i}.pickle'): \n",
    "                    with open(f'{data_dir}/tune_analysis_d{d}_s{strat}_ins{instance}_i{i}.pickle', 'rb') as handle:\n",
    "                        r = pkl.load(handle)\n",
    "                        try:\n",
    "                            results = r.get_best_result()\n",
    "                            #redo broken runs\n",
    "                            if not len(results.metrics.keys()) == 1: continue\n",
    "                            print(f'redoing broken run: tune_analysis_d{d}_s{strat}_ins{instance}_i{i}')\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                reporter = CLIReporter(max_progress_rows=10)\n",
    "                reporter.add_metric_column(\"test_mse\")\n",
    "                reporter.add_metric_column(\"test_recovery\")\n",
    "                reporter.add_metric_column(\"train_mse\")\n",
    "                reporter.add_metric_column(\"train_recovery\")\n",
    "                \n",
    "                tuner = tune.Tuner(\n",
    "                    functools.partial(train_and_evaluate, data=(datas[d]), strat_config={strat: instance}),\n",
    "                    param_space=config,\n",
    "                    run_config=RunConfig(\n",
    "                        progress_reporter=reporter\n",
    "                    ),\n",
    "                    tune_config=tune.TuneConfig(\n",
    "                        num_samples=500,\n",
    "                        scheduler=HyperBandScheduler(),\n",
    "                        metric=\"test_mse\",\n",
    "                        mode='min',\n",
    "                    ),\n",
    "                )\n",
    "                \n",
    "                results = tuner.fit()\n",
    "\n",
    "                with open(f'{data_dir}/tune_analysis_d{d}_s{strat}_ins{instance}_i{i}.pickle', 'wb') as handle:\n",
    "                    pkl.dump(results, handle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
